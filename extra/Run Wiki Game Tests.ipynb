{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ca8616",
   "metadata": {},
   "source": [
    "# Wiki Game Solver\n",
    "## Data Quality and Text Mining Capstone 2022\n",
    "Chelsea Cantone, Philip Franco, Eric Retinger\n",
    "\n",
    "12/7/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67ad52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from similarityCheck import wordScore\n",
    "from WikiWebCrawler import buildNetwork, getLinksFromTextBS, playWikiGame, recursiveSearch, createWikiGraph\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e443e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newGame(start_article, end_article, n):\n",
    "    print('Starting Game {}: {} -> {}'.format(n+1, start_article, end_article))\n",
    "\n",
    "    current_article = start_article\n",
    "    \n",
    "    models = ['L6', 'L12', 'microsoftNet', 'bert', 'roberta']\n",
    "    \n",
    "    result_steps = []\n",
    "    result_pages_visited = []\n",
    "    result_time = []\n",
    "    result_win = []\n",
    "\n",
    "    \n",
    "    for model in models:\n",
    "        steps, pages_visited, elapsed_time, win = playWikiGame(start_article, end_article, model)\n",
    "        print(model + \":        It took \" + str(steps) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + elapsed_time)\n",
    "        result_steps.append(steps)\n",
    "        result_pages_visited.append(pages_visited)\n",
    "        result_time.append(elapsed_time)\n",
    "        result_win.append(win)\n",
    "        \n",
    "    if not any(result_win):\n",
    "        print(\"All models failed to converge\")\n",
    "        raise Exception(\"All models failed to converge\")\n",
    "    \n",
    "    # Find true distances\n",
    "    tic = time.perf_counter()\n",
    "    data = recursiveSearch(5, [start_article], {'source': [], 'target': [], 'weight': []}, end_article)\n",
    "    toc = time.perf_counter()\n",
    "    time_search = toc-tic\n",
    "    search_win = time_search < 120\n",
    "    print('Time to search was: {}'.format(time_search))\n",
    "\n",
    "    # Create Network Graph\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    pages_visited_search = df.source.unique()\n",
    "    steps_search = len(df.source.unique())\n",
    "    # Returns steps to true answer\n",
    "    steps_true = buildNetwork(df, start_article, end_article)\n",
    "    \n",
    "    models.append('BFS')\n",
    "    result_steps.append(steps_search)\n",
    "    result_time.append(time_search)\n",
    "    result_pages_visited.append(pages_visited_search.tolist())\n",
    "    result_win.append(search_win)\n",
    "\n",
    "    # Create Dictionary\n",
    "    results_dict = {'Model': models,\n",
    "                   'Steps': result_steps,\n",
    "                   'Time': result_time,\n",
    "                   'Actual_Dist': [len(steps_true)-1] * len(models),\n",
    "                   'Paths_Visited': result_pages_visited,\n",
    "                   'Win': result_win}    \n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4920bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageViews(start_article):\n",
    "    start_article_formatted = start_article.replace(\" \",\"_\")\n",
    "    start_article_formatted\n",
    "    links = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/user/{}/monthly/20221101/20221201\".format(start_article_formatted)\n",
    "\n",
    "    header={\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(links, headers=header).json()\n",
    "    page_views = response['items'][0]['views']\n",
    "    return page_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1e1415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===============================================================\n",
      "Louisville City FC : 7280 views\n",
      "Kwon Nara : 22319 views\n",
      "Starting Game 1: Louisville City FC -> Kwon Nara\n",
      "FAILED!!!!\n",
      "\n",
      "\n",
      "===============================================================\n",
      "The Package (1989 film) : 2551 views\n",
      "Kim Min-ha : 14316 views\n",
      "Starting Game 2: The Package (1989 film) -> Kim Min-ha\n"
     ]
    }
   ],
   "source": [
    "playGameNTimes = 100\n",
    "\n",
    "keys = ['Model', 'Steps', 'Time', 'Actual_Dist', 'Win']\n",
    "\n",
    "for i in range(0,playGameNTimes):\n",
    "    print('\\n\\n===============================================================')\n",
    "    final_results = {'Model': [],\n",
    "                'Steps': [],\n",
    "                'Time': [],\n",
    "                'Actual_Dist': [],\n",
    "                'Paths_Visited':[],\n",
    "                'Win': []}\n",
    "    \n",
    "    # Generate two random pages\n",
    "    views = 0\n",
    "    \n",
    "    while views < 1000:\n",
    "        page = requests.get(\"https://en.wikipedia.org/api/rest_v1/page/random/summary\").json()\n",
    "        start_article = page['title']\n",
    "        views = getPageViews(start_article)\n",
    "    print('{} : {} views'.format(start_article, views))\n",
    "\n",
    "    views = 0\n",
    "\n",
    "    while views < 10000:\n",
    "        page = requests.get(\"https://en.wikipedia.org/api/rest_v1/page/random/summary\").json()\n",
    "        end_article = page['title']\n",
    "        views = getPageViews(end_article)\n",
    "    print('{} : {} views'.format(end_article, views))\n",
    "        \n",
    "    try:\n",
    "        # Run game\n",
    "        final_results = newGame(start_article, end_article, i)\n",
    "\n",
    "    #     final_results = {k: final_results[k] + results_dict[k] for k in keys}\n",
    "\n",
    "        # Save dictionary\n",
    "        df = pd.DataFrame(final_results)\n",
    "        df.to_csv('C:\\\\Users\\\\chels\\\\PycharmProjects\\\\TextMining\\\\FinalProject\\\\WikiGameSolver\\\\game{}_{}_{}.csv' \n",
    "                  .format(i, start_article.replace(' ', '_'), end_article.replace(' ', '_')))\n",
    "        print('DONE')\n",
    "        \n",
    "    except:\n",
    "        print('FAILED!!!!')\n",
    "        continue\n",
    "        \n",
    "    print('===============================================================\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54cc5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
