{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE WIKI GAME\n",
    "# https://www.thewikigame.com\n",
    "#\n",
    "# A player starts on a given wikipedia page, and has to navigate to another wikipedia page only be clicking the hyperlinks within the article.\n",
    "# The code below can help a player navigate quickly from one page to another by using various transformer models\n",
    "\n",
    "# (2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbd41da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# articles to start and end on\n",
    "# Feel free to change them :D\n",
    "\n",
    "start_article = 'Amazon River'\n",
    "end_article = 'Emotion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80562fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinksFromTextBS(start_article):\n",
    "    page_titles = []\n",
    "    url = 'https://en.wikipedia.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'parse',\n",
    "        'page': start_article,\n",
    "        'format': 'json',\n",
    "        'prop': 'text',\n",
    "        'redirects': ''\n",
    "    }\n",
    "\n",
    "    filter_sections = ['See also',\n",
    "                       'References',\n",
    "                       'External links',\n",
    "                       'Further reading',\n",
    "                       'Notes']\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    raw_html = data['parse']['text']['*']\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "    # Get all the section names\n",
    "    allSections = soup.find_all(class_='mw-headline')\n",
    "\n",
    "    sectionNames = []\n",
    "\n",
    "    for section in allSections:\n",
    "        sectionNames.append(section.get_text())\n",
    "\n",
    "    sectionNames = [x for x in sectionNames if x not in filter_sections]\n",
    "\n",
    "    # Get links from summary section up to the Table of Contents\n",
    "    target = soup.find(class_='mw-parser-output')\n",
    "    if target:\n",
    "        for sib in target.find_all_next():\n",
    "            # Only look in current section, end if hitting next section\n",
    "            # print(sib)\n",
    "            if sib.name == \"h2\":\n",
    "                break\n",
    "            elif 'title' in sib.attrs and 'Edit this' in sib.attrs['title']:\n",
    "                # Don't include the hrefs to edit the pages\n",
    "                continue\n",
    "            else:\n",
    "                # Check if href contains internal /wiki/ path\n",
    "                check1 = 'href' in sib.attrs and \\\n",
    "                         '/wiki/' in sib.attrs['href']\n",
    "                # Check if tag contains class mw-redirect\n",
    "                check2 = 'class' in sib.attrs and \\\n",
    "                         'mw-redirect' in sib.attrs['class']\n",
    "                if (check1 or check2) and 'title' in sib.attrs and 'wiktionary' not in sib.attrs['title']:\n",
    "                    page_titles.append(sib.attrs['title'])\n",
    "\n",
    "    # Get all the links from each of the relevant sections\n",
    "    for thisSection in sectionNames:\n",
    "        # print('==--------' + thisSection + '--------==')\n",
    "        target = soup.find(class_='mw-headline', id=thisSection.replace(' ', '_'))\n",
    "        if target:\n",
    "            for sib in target.find_all_next():\n",
    "                # Only look in current section, end if hitting next section\n",
    "                # print(sib)\n",
    "                if sib.name == \"h2\":\n",
    "                    break\n",
    "                else:\n",
    "                    # Check if href contains internal /wiki/ path\n",
    "                    check1 = 'href' in sib.attrs and \\\n",
    "                    '/wiki/' in sib.attrs['href']\n",
    "                    # Check if tag contains class mw-redirect\n",
    "                    check2 = 'class' in sib.attrs and \\\n",
    "                        'mw-redirect' in sib.attrs['class']\n",
    "                    if (check1 or check2) and 'title' in sib.attrs:\n",
    "                        page_titles.append(sib.attrs['title'])\n",
    "    # Unique pages only\n",
    "    page_titles = list(set(page_titles))\n",
    "    return page_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fe434a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kawahíb people', 'Iquitos, Peru', 'Indo-Iranian languages', 'Ecology', 'Solimões', 'Casiquiare canal', 'Muisca economy', 'Leticia, Amazonas', 'Óbidos, Brazil', 'Inca Empire', 'Amazons', 'Pororoca', 'Pinzón Island', 'Tonantins River', 'Freshwater fish', 'Bay', 'Ecosystem', 'Agriculture', 'National Geographic', 'Estuary', 'Nile', 'British English', 'Marañón River', 'Ecosystem collapse', 'Amazônia National Park', 'Quito', 'Orinoco', 'Ticuna', 'Lake Titicaca', 'Huallaga River', 'Pardo', 'Resin', 'Breves, Pará', 'Colonization', 'White Brazilian', 'List of rivers by discharge', 'Pracuúba', 'Ethnonym', 'Catfish', 'Meeting of Waters', 'Cretaceous', 'Tambaqui', 'Anabranch', 'Sea level', 'Piranha', 'Juruá River', 'Pre-Columbian era', 'Geology', 'Bridge', 'Juruá', 'Category:Birds of the Amazon rainforest', 'Caquetá River', 'Carl Friedrich Philipp von Martius', 'International Union for Conservation of Nature', 'Midden', 'Category:Trees of the Amazon rainforest', 'Negro River (Amazon)', 'Vaupés River', 'Peru', 'Karma', 'International Scale of River Difficulty', 'Atuá River', 'Maranhão', 'Tambo River (Peru)', 'Jupati River', 'Miocene', 'Malnutrition', 'Madre de Dios River', 'Forest ecology', 'Terra preta', 'Napo River', 'The Naturalist on the River Amazons', 'Category:Fauna of the Amazon', 'Munduruku', 'Andean civilizations', 'Terrace (agriculture)', 'Ferry', 'Bull shark', 'Nevado Mismi', 'Office of the United Nations High Commissioner for Human Rights', 'Solimões River', 'Tocantins River', 'River dolphin', 'Paru River', 'Three Gorges Dam', 'Iranian peoples', 'Brazil nut', 'Gondwana', 'Freshwater Angelfish', 'Lake Junin', 'Thermoproteota', 'Arequipa Region', 'Archaeology', 'Johann Baptist von Spix', 'Kumakuma', 'Typhus', 'Electric eel', 'Scythians', 'Ecuador', 'Glacial period', 'Spanish language', 'List of river systems by length', 'Araguaia River', 'Manacapuru River', 'Atlantic Ocean', 'Paracauari River', 'Coari River', 'Vila Nova River', 'Igapó-Açu River', 'Scholarly peer review', 'Samuel Fritz', 'Freshwater', 'Archaeologist', 'Encyclopædia Britannica Eleventh Edition', 'Curuá Una River', 'Santarém, Brazil', 'Animal echolocation', 'Jutai River', 'Steamboat', 'Brazilian Institute of Geography and Statistics', 'Cacao bean', 'Hesychius of Alexandria', 'Jari River', 'Public domain', 'NASA', 'Tefé River', 'Maicuru River', 'Salinity', 'Tribal warfare', 'Alexander von Humboldt', 'Tabatinga', 'Conquistador', 'Urubamba River', 'Uatumã River', 'Amazonas (Brazilian state)', 'Potamotrygonidae', 'Dolphin', 'Candirú', 'Amazonian manatee', 'Guajará River (Amazon)', 'Ucayali', 'Biodiversity', 'Ethnic groups in Europe', 'River bifurcation', 'Xingu River', 'Green anaconda', 'Macapá', 'Bandeirante', 'American English', 'Caiman', 'Carhuasanta', 'Amapá', 'Badajós River', 'Amazon Theatre', 'Grão-Pará Province', 'Urarina', 'Feather', 'Pará', 'Greek mythology', 'Javary River', 'Arari River', 'Natural rubber', 'Mamiá River (Amazonas)', 'Gonzalo Pizarro', 'Guaporé River', 'Tefé', 'Gurupá', 'Arapaima', 'Fur', 'El Dorado', 'Tupi language', 'Neon tetra', 'Huánuco', 'Malaria', 'São Paulo', 'Mercantile', 'Jandiatuba River', 'Japurá River', 'Orinoco River', 'Manaus', 'Trans-Amazonian Highway', 'Dry season', 'Neon Tetra', 'Draft (hull)', 'Nauta', '20th parallel south', 'Upper Amazon', 'National Institute for Space Research', 'Getúlio Vargas', 'Pedro Teixeira', 'Mantaro River', 'Trombetas River', 'Amacayacu National Park', 'Parintins', 'Formative stage', 'Manaus Iranduba Bridge', '5th parallel north', 'Sediment', 'Alphaproteobacteria', 'Nanay River', 'Betaproteobacteria', 'Pastaza River', 'Rubber boom', 'Lima', 'Metagenomics', 'Jauaru River', 'Amazon basin', 'Ancient Greece', 'Caqueta River', 'Tucuxi', 'Piorini River', 'Drainage basin', 'Gymnotiformes', 'Juruena River', 'Cabanagem', 'The Daily Telegraph', 'Wikipedia:Citation needed', 'Anajás River', 'Coca River', 'Brazil', 's:1911 Encyclopædia Britannica/Amazon', 'Purus River', 'Main stem', 'South American Plate', 'Sarmatians', 'Urubu River (Amazonas)', 'Pará River', 'Tectonic uplift', 'Manacapuru', 'List of butterflies of the Amazon River basin and the Andes', 'Neotropical fishes', 'Georg von Langsdorff', 'Giant otter', 'Anaconda', 'Guamá River', 'Colombia', 'Help:CS1 errors', 'Demise', 'Boto', 'Belém', 'Peruvians', 'Trichomycteridae', 'Apurímac River', 'Undular bore', 'Pedro II of Brazil', 'Hugh Chisholm', 'Floodplain', 'Indigenous peoples of Brazil', 'Tributary', 'Alfred Russel Wallace', 'Royal Geographical Society', 'Pongo de Manseriche', 'Area (journal)', 'Muisca Confederation', 'Characin', 'Source of the Amazon River', 'Vicente Yáñez Pinzón', 'Silver arowana', 'Iriri River', 'Curuá River (Amazon River tributary)', 'Pacific Ocean', 'Anavilhanas National Park', 'Morona', 'Chambira River', 'Wet season', 'Actinomycetota', 'Lope de Aguirre', 'Stream channel', 'Lawriqucha', 'Timeline of Amazon history', 'Andes Mountains', 'Osteoglossum bicirrhosum', 'Portuguese language', 'Help:IPA/English', 'Marajoara culture', 'Porto Velho', 'Sanskrit', 'Amazon delta', 'Itacoatiara, Amazonas', 'Chiefdom', 'Coastline paradox', 'Rio Negro (Amazon)', 'Santarém, Pará', 'Encyclopædia Britannica', 'Microorganism', 'Uarini River', 'Branco River', 'Tapajós River', 'Nazca Plate', 'Sandstone', 'Biodiversity of Colombia', 'Putumayo River', 'Pleistocene', 'Tapajós', 'Guainía River', 'Iquitos', 'Category:Flora of the Amazon', 'Trombetas', 'Almeirim, Pará', 'Puerto Francisco de Orellana', 'Freshwater swamp forest', 'Gammaproteobacteria', 'Guayana Shield', 'Cinnamon', 'Madeira River', 'Arowana', 'Headwaters', 'Distributary', 'Richard Spruce', 'Óbidos, Pará', 'James S. Olson', 'Paraná do Urariá', 'Tigre River', 'Marajó', 'Tidal bore', 'Template:Cite book', 'Confluence', 'Ucayali River', 'List of largest snakes', 'Hammock', 'Monte Alegre, Pará', 'Drought', 'Araguari River (Amapá)', 'University of Chicago', 'Spanish Empire', 'Rio Preto da Eva', 'Henry Walter Bates', 'Allpahuayo-Mishana National Reserve', 'River mouth', 'Charles Marie de La Condamine', 'Spanish conquistador', 'Aguarico River', 'Acará River', 'Wikipedia:Manual of Style/Words to watch', 'Teles Pires', 'Purús River', 'Lumber', 'Jutaí River', 'La Canela', 'Hamza River', 'Curuçá', 'Yanomami', 'Amazon rainforest', 'Pedreira River', 'Amazon Delta', 'Tributaries', 'Irineu Evangelista de Sousa', 'Francisco de Orellana', 'Evangelism', 'Nhamundá River', 'List of rivers by length', 'Canumã River', 'Arequipa', 'Amazon river dolphin', 'António Raposo Tavares', 'Andes', 'Itaya River', 'George Earl Church', 'European Union', 'Wikipedia:Please clarify', 'South America', 'Matapi River']\n"
     ]
    }
   ],
   "source": [
    "# for the following\n",
    "# start_article = 'Amazon River'\n",
    "# end_article = 'Emotion'\n",
    "\n",
    "links = getLinksFromTextBS('Amazon River')\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3269081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for Word Similarity Check\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# wordScore takes in the list of words from a wikipedia link and the name of the goal page.\n",
    "def wordScore(word1, word2, model, logs=0):\n",
    "    if model == 'bert':\n",
    "        model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    elif model == 'roberta':\n",
    "        model = SentenceTransformer('all-distilroberta-v1')\n",
    "    elif model == 'microsoftNet':\n",
    "        model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    elif model == 'L12':\n",
    "        model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "    elif model == 'L6':\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    cosine_scores = []\n",
    "\n",
    "    # Make word 1 to be an equal length to word 2\n",
    "    word1_arr = [word1] * len(word2)\n",
    "\n",
    "    embeddings1 = model.encode(word1_arr)\n",
    "    embeddings2 = model.encode(word2)\n",
    "\n",
    "    # compute cosine_scores\n",
    "    cosine_matrix = util.cos_sim(embeddings1, embeddings2)\n",
    "    cosine_scores = cosine_matrix.diagonal()\n",
    "\n",
    "    scores = cosine_scores.tolist()\n",
    "\n",
    "    if logs == 1:\n",
    "        # create dataframe\n",
    "        df_wiki = pd.DataFrame(list(zip(word2, scores)),\n",
    "               columns =['target', 'weight'])\n",
    "\n",
    "        # sort dataframe and reorganize index\n",
    "        df_wiki_org = df_wiki.sort_values(by=['weight'], ascending=False)\n",
    "        df_wiki_org = df_wiki_org.reset_index(drop=True)\n",
    "\n",
    "        print(df_wiki_org)\n",
    "\n",
    "    # convert the tensor array to a list and return\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0a2fa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    target    weight\n",
      "0    Karma  0.421835\n",
      "1     Pará  0.335476\n",
      "2      Fur  0.328533\n",
      "3  Feather  0.328417\n",
      "4   Bridge  0.317793\n"
     ]
    }
   ],
   "source": [
    "# for the following\n",
    "# start_article = 'Amazon River'\n",
    "# end_article = 'Emotion'\n",
    "\n",
    "word1 = 'Emotion'\n",
    "word2 = ['Karma', 'Pará', 'Fur', 'Feather', 'Bridge']\n",
    "model = \"L6\"\n",
    "scores = wordScore(word1, word2, model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b596dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check page views\n",
    "# If a wiki page didnt have a lot of page veiws, ignore the game.\n",
    "\n",
    "def getPageViews(start_article):\n",
    "    start_article_formatted = start_article.replace(\" \",\"_\")\n",
    "    start_article_formatted\n",
    "    links = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/user/{}/monthly/20221101/20221201\".format(start_article_formatted)\n",
    "\n",
    "    header={\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(links, headers=header).json()\n",
    "    page_views = response['items'][0]['views']\n",
    "    return page_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26b7f83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60968\n"
     ]
    }
   ],
   "source": [
    "print(getPageViews('Karma'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6590ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playWikiGame(head, final, model):\n",
    "    steps = 0\n",
    "    pages = [head]\n",
    "    pages_visited = []\n",
    "\n",
    "    tic = time.perf_counter()\n",
    "\n",
    "    # forces the words to be the same case\n",
    "    while not str.lower(head) == str.lower(final):\n",
    "        result_links = getLinksFromTextBS(head)\n",
    "        len_target = len(final)\n",
    "\n",
    "        scores = wordScore(final, result_links, model)\n",
    "\n",
    "        # create dataframe\n",
    "        df_wiki = pd.DataFrame(list(zip(result_links, scores)),\n",
    "               columns =['target', 'weight'])\n",
    "\n",
    "        # sort dataframe and reorganize index\n",
    "        df_wiki_org = df_wiki.sort_values(by=['weight'], ascending=False)\n",
    "        df_wiki_org = df_wiki_org.reset_index(drop=True)\n",
    "\n",
    "        # if the page has already been searched, drop it from the current links\n",
    "        while df_wiki_org['target'][0] in pages:\n",
    "            df_wiki_org = df_wiki_org.drop(labels=0, axis=0)\n",
    "            df_wiki_org = df_wiki_org.reset_index(drop=True)\n",
    "\n",
    "        head = df_wiki_org['target'][0]\n",
    "        pages.append(head)\n",
    "        print(head)\n",
    "        steps = steps + 1\n",
    "        pages_visited.append(head)\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "\n",
    "    print(model + \" took \" + str(toc - tic) + \" seconds to complete\")\n",
    "    return steps, pages_visited, str(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bae87ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running MiniLM-L6\n",
      "Karma\n",
      "Psychoanalysis\n",
      "Regressive emotionality\n",
      "Emotion\n",
      "L6 took 8.705769099999998 seconds to complete\n",
      "L6: It took 4 links to get from Amazon River to Emotion\n",
      "\n",
      "Running MiniLM-L12\n",
      "Karma\n",
      "Oceanic feeling\n",
      "Positive psychology\n",
      "Emotions\n",
      "Negative emotion\n",
      "Emotion\n",
      "L12 took 15.757753100000059 seconds to complete\n",
      "L12: It took 6 links to get from Amazon River to Emotion\n",
      "\n",
      "Running MPNet\n",
      "Karma\n",
      "Spirituality\n",
      "Happiness\n",
      "Emotion\n",
      "microsoftNet took 30.05556189999993 seconds to complete\n",
      "Microsoft: It took 4 links to get from Amazon River to Emotion\n",
      "\n",
      "Running Bert\n",
      "Sediment\n",
      "Concretion\n",
      "Lithification\n",
      "Pressure\n",
      "Force\n",
      "Motion\n",
      "Vibration\n",
      "Friction\n",
      "Energy\n",
      "Atmosphere\n",
      "Terrain\n",
      "Fluvial\n",
      "Flow velocity\n",
      "Velocity\n",
      "Area\n",
      "Shapes\n",
      "Material\n",
      "Forces\n",
      "Dynamic pressure\n",
      "Irrotational flow\n",
      "Compressible flow\n",
      "Compressibility\n",
      "Fluid\n",
      "Matter\n",
      "Mass\n",
      "Resonance\n",
      "Attenuation\n",
      "Phenomenon\n",
      "Phenomenalism\n",
      "Phenomena\n",
      "Observation\n",
      "Perception\n",
      "Emotions\n",
      "Emotional expression\n",
      "Emotional intelligence\n",
      "Emotion\n",
      "bert took 138.31370530000004 seconds to complete\n",
      "Bert: It took 36 links to get from Amazon River to Emotion\n",
      "\n",
      "Running Roberta\n",
      "Ecosystem\n",
      "Energy\n",
      "Energetic (disambiguation)\n",
      "Energetic mood\n",
      "Sensory evidential mood\n",
      "Modality (natural language)\n",
      "Indicative mood\n",
      "Imperative mood\n",
      "Presumptive mood\n",
      "Potential mood\n",
      "Permissive mood\n",
      "Optative mood\n",
      "Desiderative mood\n",
      "Linguistics\n",
      "Cognition\n",
      "Emotion\n",
      "roberta took 35.45407399999988 seconds to complete\n",
      "Roberta: It took 16 links to get from Amazon River to Emotion\n",
      "\n",
      "L6:        It took 4 links to get from Amazon River to Emotion in 8.705769099999998\n",
      "L12:       It took 6 links to get from Amazon River to Emotion in 15.757753100000059\n",
      "Microsoft: It took 4 links to get from Amazon River to Emotion in 30.05556189999993\n",
      "Bert:      It took 36 links to get from Amazon River to Emotion in 138.31370530000004\n",
      "Roberta:   It took 16 links to get from Amazon River to Emotion in 35.45407399999988\n"
     ]
    }
   ],
   "source": [
    "# Identify Starting Page\n",
    "#start_article = 'Amazon River'\n",
    "#end_article = 'Emotion'\n",
    "\n",
    "current_article = start_article\n",
    "\n",
    "print(\"\\nRunning MiniLM-L6\")\n",
    "steps_l6, pages_visited_l6, time_l6 = playWikiGame(start_article, end_article, 'L6')\n",
    "print(\"L6: It took \" + str(steps_l6) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nRunning MiniLM-L12\")\n",
    "steps_l12, pages_visited_l12, time_l12 = playWikiGame(start_article, end_article, 'L12')\n",
    "print(\"L12: It took \" + str(steps_l12) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nRunning MPNet\")\n",
    "steps_micro, pages_visited_micro, time_micro = playWikiGame(start_article, end_article, 'microsoftNet')\n",
    "print(\"Microsoft: It took \" + str(steps_micro) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nRunning Bert\")\n",
    "steps_bert, pages_visited_bert, time_bert = playWikiGame(start_article, end_article, 'bert')\n",
    "print(\"Bert: It took \" + str(steps_bert) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nRunning Roberta\")\n",
    "steps_roberta, pages_visited, time_roberta = playWikiGame(start_article, end_article, 'roberta')\n",
    "print(\"Roberta: It took \" + str(steps_roberta) + \" links to get from \" + start_article + \" to \" + end_article)\n",
    "\n",
    "print(\"\\nL6:        It took \" + str(steps_l6) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_l6)\n",
    "print(\"L12:       It took \" + str(steps_l12) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_l12)\n",
    "print(\"Microsoft: It took \" + str(steps_micro) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_micro)\n",
    "print(\"Bert:      It took \" + str(steps_bert) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_bert)\n",
    "print(\"Roberta:   It took \" + str(steps_roberta) + \" links to get from \" + start_article + \" to \" + end_article +  \" in \" + time_roberta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
